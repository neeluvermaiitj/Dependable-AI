{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E. Accuracy after bias detection and mitigation on cipher10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neeluvermaiitj/Dependable-AI/blob/main/E_Accuracy_after_bias_detection_and_mitigation_on_cipher10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fmjJbCADjIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6604f7f0-360e-41fb-b988-b851b4acacd8"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8OPakEz6OkL",
        "outputId": "13b779c7-3750-478c-fefa-bff68df18b95"
      },
      "source": [
        "  # Complete dataset \n",
        "  import keras\n",
        "  from keras import layers\n",
        "  from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, MaxPool2D, Flatten, BatchNormalization,Input,MaxPooling2D,Dense,Flatten,Dropout\n",
        "  from keras.layers import Conv1D, MaxPool1D, Reshape\n",
        "  from keras.layers import Input, Dense, Dropout, Activation, Add, Concatenate\n",
        "  from keras.models import Model, Sequential,load_model\n",
        "  from keras.datasets import cifar10\n",
        "  import tensorflow as tf\n",
        "  from keras.optimizers import SGD, Adam, RMSprop, Adadelta\n",
        "  from keras.utils import to_categorical\n",
        "  import scipy.io as sio\n",
        "  from scipy.io import loadmat\n",
        "  import random\n",
        "  from keras.objectives import mean_squared_error\n",
        "  import keras.backend as K\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  from sklearn import datasets, svm, metrics\n",
        "  from keras.datasets import cifar10\n",
        "\n",
        "  def shuffle_in_unison(a, b):\n",
        "      assert len(a) == len(b)\n",
        "      shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
        "      shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
        "      permutation = np.random.permutation(len(a))\n",
        "      for old_index, new_index in enumerate(permutation):\n",
        "          shuffled_a[new_index] = a[old_index]\n",
        "          shuffled_b[new_index] = b[old_index]\n",
        "      return shuffled_a, shuffled_b\n",
        "  # scale pixels    \n",
        "  def prep_pixels(train, test):\n",
        "    # convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    # normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    # return normalized images\n",
        "    return train_norm, test_norm \n",
        "  def load_dataset():\n",
        "    # load dataset\n",
        "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "    # one hot encode target values\n",
        "    trainY = to_categorical(trainY)\n",
        "    testY = to_categorical(testY)\n",
        "    return trainX, trainY,testX, testY\n",
        "\n",
        "  # define cnn model\n",
        "  def define_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(32, 32, 3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    # compile model\n",
        "    opt = SGD(lr=0.005, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "  \n",
        "  def create_dataset():  \n",
        "    \n",
        "    if load_dataset_saved == 0:\n",
        "        # load dataset\n",
        "        trainX, trainY, testX, testY = load_dataset()\n",
        "        # prepare pixel data\n",
        "        trainX, testX = prep_pixels(trainX, testX)\n",
        "\n",
        "        MAT_save_name = 'trainXlinf_cnn_pert.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        annots = loadmat(path)\n",
        "        annots.keys()  \n",
        "        trainXlinf = annots['mydata']\n",
        "        \n",
        "        MAT_save_name = 'fgsm_cnn_pert_train.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        annots = loadmat(path)\n",
        "        annots.keys()\n",
        "        trainXFGSM = annots['mydata']\n",
        "\n",
        "        trainX = np.concatenate([np.array(trainX), np.array(trainXFGSM), np.array(trainXlinf)])\n",
        "        #trainX = np.concatenate([np.array(trainX), np.array(trainXlinf)])\n",
        "        trainY = np.concatenate([np.array(trainY), np.array(trainY), np.array(trainY)])\n",
        "\n",
        "        MAT_save_name = 'fgsm_cnn_pert.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        annots = loadmat(path)\n",
        "        annots.keys()  \n",
        "        testXadv = annots['mydata']\n",
        "        \n",
        "        MAT_save_name = 'testXlinf_cnn_pert.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        annots = loadmat(path)\n",
        "        annots.keys()  \n",
        "        testXlinf = annots['mydata']\n",
        "\n",
        "        testX = np.concatenate([np.array(testX), np.array(testXadv), np.array(testXlinf)])\n",
        "        testY = np.concatenate([np.array(testY), np.array(testY), np.array(testY)])\n",
        "\n",
        "        MAT_save_name = 'RetrainX.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        sio.savemat(path, {'mydata': trainX})\n",
        "\n",
        "        MAT_save_name = 'RetrainY.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        sio.savemat(path, {'mydata': trainY})\n",
        "        \n",
        "        MAT_save_name = 'RetestX.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        sio.savemat(path, {'mydata': testX})\n",
        "\n",
        "        MAT_save_name = 'RetestY.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        sio.savemat(path, {'mydata': testY})\n",
        "\n",
        "        print(\"saved dataset\")\n",
        "\n",
        "    else:\n",
        "        MAT_save_name = 'RetrainX.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        annots = loadmat(path)\n",
        "        annots.keys()\n",
        "        trainX = annots['mydata']\n",
        "\n",
        "        MAT_save_name = 'RetrainY.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        annots = loadmat(path)\n",
        "        annots.keys()\n",
        "        trainY = annots['mydata']\n",
        "\n",
        "        MAT_save_name = 'RetestX.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        annots = loadmat(path)\n",
        "        annots.keys()\n",
        "        testX = annots['mydata']\n",
        "\n",
        "        MAT_save_name = 'RetestY.mat'\n",
        "        path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "        annots = loadmat(path)\n",
        "        annots.keys()\n",
        "        testY = annots['mydata']\n",
        "    trainX, trainY = shuffle_in_unison(trainX, trainY)\n",
        "    testX, testY = shuffle_in_unison(testX, testY)\n",
        "    return trainX, trainY, testX, testY\n",
        "\n",
        "  # run the test harness for evaluating a model\n",
        "  def run_test_harness():\n",
        "    trainX, trainY, testX, testY = create_dataset()\n",
        "    print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
        "    if load_trained_model == 0:\n",
        "      # define model\n",
        "      model = define_model()\n",
        "      print(model)\n",
        "      # fit model\n",
        "      history = model.fit(trainX, trainY, epochs=10, batch_size=64, validation_data=(testX, testY), verbose=1)\n",
        "      model_save_name = 'retrained_model.h5'\n",
        "      path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{model_save_name}\" \n",
        "      model.save(path)\n",
        "    else:\n",
        "\n",
        "      model_save_name = 'retrained_model.h5'\n",
        "      path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{model_save_name}\" \n",
        "      model = load_model(path)\n",
        "    # evaluate model\n",
        "    _, acc = model.evaluate(testX, testY, verbose=1)\n",
        "    print('> %.3f' % (acc * 100.0))\n",
        "  # entry point, run the test harness\n",
        "  labels = [  'airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "  load_dataset_saved  = 1\n",
        "  load_trained_model = 0\n",
        "  run_test_harness()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150000, 32, 32, 3) (150000, 10) (30000, 32, 32, 3) (30000, 10)\n",
            "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fc8f118c630>\n",
            "Train on 150000 samples, validate on 30000 samples\n",
            "Epoch 1/10\n",
            "150000/150000 [==============================] - ETA: 0s - loss: 2.0977 - accuracy: 0.2034"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "150000/150000 [==============================] - 124s 825us/sample - loss: 2.0977 - accuracy: 0.2034 - val_loss: 1.6843 - val_accuracy: 0.3453\n",
            "Epoch 2/10\n",
            "150000/150000 [==============================] - 115s 769us/sample - loss: 1.4345 - accuracy: 0.4682 - val_loss: 1.2101 - val_accuracy: 0.5619\n",
            "Epoch 3/10\n",
            "150000/150000 [==============================] - 116s 775us/sample - loss: 1.0914 - accuracy: 0.6067 - val_loss: 1.0258 - val_accuracy: 0.6372\n",
            "Epoch 4/10\n",
            "150000/150000 [==============================] - 114s 761us/sample - loss: 0.8484 - accuracy: 0.6987 - val_loss: 0.8948 - val_accuracy: 0.6871\n",
            "Epoch 5/10\n",
            "150000/150000 [==============================] - 114s 758us/sample - loss: 0.6535 - accuracy: 0.7706 - val_loss: 0.9025 - val_accuracy: 0.6899\n",
            "Epoch 6/10\n",
            "150000/150000 [==============================] - 115s 767us/sample - loss: 0.4832 - accuracy: 0.8309 - val_loss: 0.9960 - val_accuracy: 0.6955\n",
            "Epoch 7/10\n",
            "150000/150000 [==============================] - 115s 767us/sample - loss: 0.3576 - accuracy: 0.8742 - val_loss: 0.9357 - val_accuracy: 0.7203\n",
            "Epoch 8/10\n",
            "150000/150000 [==============================] - 115s 764us/sample - loss: 0.2549 - accuracy: 0.9112 - val_loss: 0.9966 - val_accuracy: 0.7143\n",
            "Epoch 9/10\n",
            "150000/150000 [==============================] - 113s 756us/sample - loss: 0.1884 - accuracy: 0.9354 - val_loss: 1.0857 - val_accuracy: 0.7183\n",
            "Epoch 10/10\n",
            "150000/150000 [==============================] - 110s 733us/sample - loss: 0.1450 - accuracy: 0.9499 - val_loss: 1.0960 - val_accuracy: 0.7226\n",
            "> 72.260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y7pXyrdPIR8",
        "outputId": "ce70504f-b6de-4b53-d9c7-1bf7f28dc2ee"
      },
      "source": [
        "\n",
        "\n",
        "  # Complete dataset \n",
        "  import keras\n",
        "  from keras import layers\n",
        "  from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, MaxPool2D, Flatten, BatchNormalization,Input,MaxPooling2D,Dense,Flatten,Dropout\n",
        "  from keras.layers import Conv1D, MaxPool1D, Reshape\n",
        "  from keras.layers import Input, Dense, Dropout, Activation, Add, Concatenate\n",
        "  from keras.models import Model, Sequential,load_model\n",
        "  from keras.datasets import cifar10\n",
        "  import tensorflow as tf\n",
        "  from keras.optimizers import SGD, Adam, RMSprop, Adadelta\n",
        "  from keras.utils import to_categorical\n",
        "  import scipy.io as sio\n",
        "  from scipy.io import loadmat\n",
        "  import random\n",
        "  from keras.objectives import mean_squared_error\n",
        "  import keras.backend as K\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  from sklearn import datasets, svm, metrics\n",
        "  from keras.datasets import cifar10\n",
        "\n",
        "  # scale pixels    \n",
        "  def prep_pixels(train, test):\n",
        "    # convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    # normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    # return normalized images\n",
        "    return train_norm, test_norm \n",
        "  def load_dataset():\n",
        "    # load dataset\n",
        "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "    # one hot encode target values\n",
        "    trainY = to_categorical(trainY)\n",
        "    testY = to_categorical(testY)\n",
        "    return trainX, trainY,testX, testY\n",
        "\n",
        "  # define cnn model\n",
        "  def define_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(input_shape=(32, 32, 3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    # compile model\n",
        "    opt = SGD(lr=0.005, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "  \n",
        "  # run the test harness for evaluating a model\n",
        "  def run_test_harness():\n",
        "      # load dataset\n",
        "      trainX, trainY, testX, testY = load_dataset()\n",
        "      # prepare pixel data\n",
        "      trainX, testX = prep_pixels(trainX, testX)\n",
        "      \n",
        "      model_save_name = 'retrained_model.h5'\n",
        "      path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{model_save_name}\" \n",
        "      model = load_model(path)\n",
        "\n",
        "      # evaluate model\n",
        "      _, acc = model.evaluate(testX, testY, verbose=1)\n",
        "      print(' Performance on Oriinal Dataset > %.3f' % (acc * 100.0))\n",
        "\n",
        "      MAT_save_name = 'fgsm_cnn_pert.mat'\n",
        "      path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "      annots = loadmat(path)\n",
        "      annots.keys()\n",
        "      testX = annots['mydata']\n",
        "\n",
        "      # evaluate model\n",
        "      _, acc = model.evaluate(testX, testY, verbose=1)\n",
        "      print(' Performance on FGSM Dataset > %.3f' % (acc * 100.0))\n",
        "\n",
        "      MAT_save_name = 'testXlinf_cnn_pert.mat'\n",
        "      path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "      annots = loadmat(path)\n",
        "      annots.keys()\n",
        "      testX = annots['mydata']\n",
        "\n",
        "      # evaluate model\n",
        "      _, acc = model.evaluate(testX, testY, verbose=1)\n",
        "      print(' Performance on L-Inf Dataset > %.3f' % (acc * 100.0))\n",
        "\n",
        "  run_test_harness()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 105s 335ms/step - loss: 1.0293 - accuracy: 0.7390\n",
            " Performance on Oriinal Dataset > 73.900\n",
            "313/313 [==============================] - 105s 336ms/step - loss: 1.2213 - accuracy: 0.6908\n",
            " Performance on FGSM Dataset > 69.080\n",
            "313/313 [==============================] - 105s 334ms/step - loss: 1.0373 - accuracy: 0.7380\n",
            " Performance on L-Inf Dataset > 73.800\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}