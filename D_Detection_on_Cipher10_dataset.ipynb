{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "D.  Detection on Cipher10 dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neeluvermaiitj/Dependable-AI/blob/main/D_Detection_on_Cipher10_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah1eLcbazD-F"
      },
      "source": [
        "!pip install adversarial-robustness-toolbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsBan_C6BjSh"
      },
      "source": [
        "# Perform detection using new CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngjl5AUVBior"
      },
      "source": [
        "#from  art.defences.detector.evasion  import BinaryInputDetector\n",
        "#from art.estimators.classification import KerasClassifier\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from scipy.io import loadmat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yJmeBUNBqHw"
      },
      "source": [
        "def shuffle_in_unison(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
        "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
        "    permutation = np.random.permutation(len(a))\n",
        "    for old_index, new_index in enumerate(permutation):\n",
        "        shuffled_a[new_index] = a[old_index]\n",
        "        shuffled_b[new_index] = b[old_index]\n",
        "    return shuffled_a, shuffled_b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB6LaJkWPfpH",
        "outputId": "3ae0fc3f-bba2-479d-a568-c2d5694f9a02"
      },
      "source": [
        "# define cnn model\n",
        "def define_detector_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(input_shape=(32, 32, 3),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  #model.BatchNormalization(),\n",
        "  model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  # compile model\n",
        "  opt = SGD(lr=0.001, momentum=0.9)\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  #model.compile(optimizer=opt, loss='binary_categorical', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm \n",
        "def load_dataset():\n",
        "  # load dataset\n",
        "  (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "  # one hot encode target values\n",
        "  trainY = to_categorical(trainY)\n",
        "  testY = to_categorical(testY)\n",
        "  return trainX, trainY,testX, testY\n",
        "def run_test_harness():\n",
        "  MAT_save_name = 'trainXSVM_complete_data.mat'\n",
        "  path = F\"/content/gdrive/My Drive/{MAT_save_name}\"\n",
        "  annots = loadmat(path)\n",
        "  annots.keys()  \n",
        "  trainXSVM = annots['mydata']\n",
        "\n",
        "  MAT_save_name = 'trainYSVM_complete_data.mat'\n",
        "  path = F\"/content/gdrive/My Drive/{MAT_save_name}\"\n",
        "  annots = loadmat(path)\n",
        "  annots.keys()  \n",
        "  trainYSVM = annots['mydata']\n",
        "\n",
        "  MAT_save_name = 'testXSVM_complete_data.mat'\n",
        "  path = F\"/content/gdrive/My Drive/{MAT_save_name}\"\n",
        "  annots = loadmat(path)\n",
        "  annots.keys()  \n",
        "  testXSVM = annots['mydata']\n",
        "\n",
        "  MAT_save_name = 'testYSVM_complete_data.mat'\n",
        "  path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "  annots = loadmat(path)\n",
        "  annots.keys()  \n",
        "  testYSVM = annots['mydata']\n",
        "\n",
        "  trainYSVM = trainYSVM.reshape(150000,1)\n",
        "  trainXSVM, trainYSVM = shuffle_in_unison(trainXSVM, trainYSVM)\n",
        "  testXSVM,testYSVM = shuffle_in_unison(testXSVM, testYSVM)\n",
        "\n",
        "  #print(trainXSVM.shape , trainYSVM.shape , testXSVM.shape , testYSVM,shape)\n",
        "\n",
        "  #print(error)\n",
        "  trainXSVM = trainXSVM[:1000]\n",
        "  trainYSVM = trainYSVM[:1000]\n",
        "\n",
        "  testXSVM = trainXSVM[:100]\n",
        "  testYSVM = testYSVM[:100]\n",
        "  \n",
        "  trainYSVM = to_categorical(trainYSVM)\n",
        "  testYSVM = to_categorical(testYSVM)\n",
        "  \n",
        "  if load_trained_detector_model == 0:\n",
        "    # define model\n",
        "    detector_model = define_detector_model()\n",
        "    detector_model.summary()\n",
        "    # fit model\n",
        "\n",
        "    history = detector_model.fit(trainXSVM, trainYSVM, epochs=10, batch_size=64, validation_data=(testXSVM, testYSVM), verbose=1)\n",
        "    model_save_name = 'detector_model.h5'\n",
        "    path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{model_save_name}\" \n",
        "    detector_model.save(path)\n",
        "  else:\n",
        "    model_save_name = 'detector_model.h5'\n",
        "    path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{model_save_name}\"  \n",
        "    detector_model = load_model(path)\n",
        "\n",
        "  # evaluate model\n",
        "  _, acc = detector_model.evaluate(testXSVM, testYSVM, verbose=1)\n",
        "  print('> %.3f' % (acc * 100.0))\n",
        "\n",
        "# entry point, run the test harness\n",
        "labels = [  'airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "load_trained_detector_model=0\n",
        "run_test_harness()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 2, 2, 128)         295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 2, 2, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 1,545,954\n",
            "Trainable params: 1,543,778\n",
            "Non-trainable params: 2,176\n",
            "_________________________________________________________________\n",
            "Train on 1000 samples, validate on 100 samples\n",
            "Epoch 1/10\n",
            "1000/1000 [==============================] - ETA: 0s - loss: 0.7109 - accuracy: 0.5880"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 2s 2ms/sample - loss: 0.7109 - accuracy: 0.5880 - val_loss: 0.6754 - val_accuracy: 0.7400\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 0s 443us/sample - loss: 0.6341 - accuracy: 0.6820 - val_loss: 0.6711 - val_accuracy: 0.7400\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 0s 451us/sample - loss: 0.5900 - accuracy: 0.6890 - val_loss: 0.6614 - val_accuracy: 0.7400\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 0s 493us/sample - loss: 0.5269 - accuracy: 0.7360 - val_loss: 0.6587 - val_accuracy: 0.7400\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 0s 448us/sample - loss: 0.4768 - accuracy: 0.7880 - val_loss: 0.6470 - val_accuracy: 0.7400\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 0s 465us/sample - loss: 0.3904 - accuracy: 0.8390 - val_loss: 0.6418 - val_accuracy: 0.7400\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 0s 448us/sample - loss: 0.3146 - accuracy: 0.8970 - val_loss: 0.6383 - val_accuracy: 0.7400\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 0s 460us/sample - loss: 0.2178 - accuracy: 0.9500 - val_loss: 0.6338 - val_accuracy: 0.7400\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 0s 447us/sample - loss: 0.1249 - accuracy: 0.9820 - val_loss: 0.6185 - val_accuracy: 0.7400\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 0s 459us/sample - loss: 0.0717 - accuracy: 0.9930 - val_loss: 0.6197 - val_accuracy: 0.7500\n",
            "> 75.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZtle9OsB45I"
      },
      "source": [
        "# Perform detection using new CNN2 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNzWlm_PT46x",
        "outputId": "b4afe539-4b4f-4919-ffb4-75f68b29fb68"
      },
      "source": [
        "\n",
        "\n",
        "#from  art.defences.detector.evasion  import BinaryInputDetector\n",
        "#from art.estimators.classification import KerasClassifier\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from scipy.io import loadmat\n",
        "\n",
        "def shuffle_in_unison(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
        "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
        "    permutation = np.random.permutation(len(a))\n",
        "    for old_index, new_index in enumerate(permutation):\n",
        "        shuffled_a[new_index] = a[old_index]\n",
        "        shuffled_b[new_index] = b[old_index]\n",
        "    return shuffled_a, shuffled_b\n",
        "# define cnn model\n",
        "\n",
        "def define_detector_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(input_shape=(32, 32, 3),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  #model.BatchNormalization(),\n",
        "  model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  # compile model\n",
        "  opt = SGD(lr=0.001, momentum=0.9)\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  #model.compile(optimizer=opt, loss='binary_categorical', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm \n",
        "def load_dataset():\n",
        "  # load dataset\n",
        "  (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "  # one hot encode target values\n",
        "  trainY = to_categorical(trainY)\n",
        "  testY = to_categorical(testY)\n",
        "  return trainX, trainY,testX, testY\n",
        "def run_test_harness():\n",
        "  MAT_save_name = 'trainXSVM_complete_data.mat'\n",
        "  path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "  annots = loadmat(path)\n",
        "  annots.keys()  \n",
        "  trainXSVM = annots['mydata']\n",
        "\n",
        "  MAT_save_name = 'trainYSVM_complete_data.mat'\n",
        "  path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "  annots = loadmat(path)\n",
        "  annots.keys()  \n",
        "  trainYSVM = annots['mydata']\n",
        "\n",
        "  MAT_save_name = 'testXSVM_complete_data.mat'\n",
        "  path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "  annots = loadmat(path)\n",
        "  annots.keys()  \n",
        "  testXSVM = annots['mydata']\n",
        "\n",
        "  MAT_save_name = 'testYSVM_complete_data.mat'\n",
        "  path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{MAT_save_name}\"\n",
        "  annots = loadmat(path)\n",
        "  annots.keys()  \n",
        "  testYSVM = annots['mydata']\n",
        "\n",
        "  trainYSVM = trainYSVM.reshape(150000,1)\n",
        "  trainXSVM, trainYSVM = shuffle_in_unison(trainXSVM, trainYSVM)\n",
        "  testXSVM,testYSVM = shuffle_in_unison(testXSVM, testYSVM)\n",
        "  \n",
        "  trainYSVM = to_categorical(trainYSVM)\n",
        "  testYSVM = to_categorical(testYSVM)\n",
        "  \n",
        "  if load_trained_detector_model == 0:\n",
        "    # define model\n",
        "    detector_model = define_detector_model()\n",
        "    detector_model.summary()\n",
        "    # fit model\n",
        "\n",
        "    history = detector_model.fit(trainXSVM, trainYSVM, epochs=10, batch_size=64, validation_data=(testXSVM, testYSVM), verbose=1)\n",
        "    model_save_name = 'detector_model.h5'\n",
        "    path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{model_save_name}\" \n",
        "    detector_model.save(path)\n",
        "  else:\n",
        "    model_save_name = 'detector_model.h5'\n",
        "    path = F\"/content/drive/MyDrive/Colab Notebooks/DAI_Assignment2/cifar10_code/{model_save_name}\"  \n",
        "    detector_model = load_model(path)\n",
        "\n",
        "  # evaluate model\n",
        "  _, acc = detector_model.evaluate(testXSVM, testYSVM, verbose=1)\n",
        "  print('> %.3f' % (acc * 100.0))\n",
        "\n",
        "# entry point, run the test harness\n",
        "labels = [  'airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "load_trained_detector_model=1\n",
        "run_test_harness()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "> 66.377\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}